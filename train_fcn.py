import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import numpy as np
import random
import os


# ===================== 1. æ¨¡æ‹Ÿè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ï¼ˆæ›¿ä»£Pascal VOCï¼‰ =====================
class FakeSegDataset(Dataset):
    def __init__(self, num_samples=100, img_size=(256, 256), num_classes=21):
        self.num_samples = num_samples  # æ¨¡æ‹Ÿ100å¼ è®­ç»ƒå›¾åƒ
        self.img_size = img_size  # ç»Ÿä¸€å›¾åƒå°ºå¯¸ä¸º256x256
        self.num_classes = num_classes  # 21ç±»ï¼ˆé€‚é…FCNæ ‡å‡†è¯­ä¹‰åˆ†å‰²ç±»åˆ«æ•°ï¼‰
        # å›¾åƒé¢„å¤„ç†ï¼ˆå’ŒçœŸå®æ•°æ®é›†é¢„å¤„ç†é€»è¾‘ä¸€è‡´ï¼‰
        self.transform = transforms.Compose([
            transforms.ToTensor(),  # è½¬ä¸ºTensorï¼šHWCâ†’CHWï¼Œå€¼å½’ä¸€åŒ–åˆ°0-1
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # æ ‡å‡†åŒ–
        ])

    def __len__(self):
        # æ•°æ®é›†æ€»æ•°é‡
        return self.num_samples

    def __getitem__(self, idx):
        # ç”Ÿæˆæ¨¡æ‹ŸRGBå›¾åƒï¼ˆ256x256x3ï¼Œåƒç´ å€¼0-255ï¼‰
        fake_img = np.random.randint(0, 255, size=(*self.img_size, 3), dtype=np.uint8)
        # ç”Ÿæˆæ¨¡æ‹Ÿåƒç´ çº§æ ‡ç­¾ï¼ˆ256x256ï¼Œæ¯ä¸ªåƒç´ å€¼å¯¹åº”ç±»åˆ«0-20ï¼‰
        fake_label = np.random.randint(0, self.num_classes, size=self.img_size, dtype=np.uint8)

        # é¢„å¤„ç†å›¾åƒå’Œæ ‡ç­¾
        img = self.transform(fake_img)
        label = torch.from_numpy(fake_label).long()  # æ ‡ç­¾è½¬ä¸ºlongå‹ï¼ˆé€‚é…äº¤å‰ç†µæŸå¤±ï¼‰
        return img, label


# ===================== 2. FCNè¯­ä¹‰åˆ†å‰²æ¨¡å‹æ­å»ºï¼ˆæ ¸å¿ƒç»“æ„ï¼‰ =====================
class FCN8s(nn.Module):
    def __init__(self, num_classes=21):
        super(FCN8s, self).__init__()
        # ç‰¹å¾æå–ï¼šå·ç§¯+æ± åŒ–ï¼ˆä¸‹é‡‡æ ·ï¼Œæå–é«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼‰
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),  # 3é€šé“è¾“å…¥â†’64é€šé“ç‰¹å¾
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # ä¸‹é‡‡æ ·ï¼Œå°ºå¯¸å‡åŠï¼š256â†’128
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # å°ºå¯¸ï¼š128â†’64
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # å°ºå¯¸ï¼š64â†’32
        )
        # ä¸Šé‡‡æ ·ï¼šåå·ç§¯ï¼ˆæ¢å¤åŸå›¾åƒå°ºå¯¸ï¼‰
        self.deconv = nn.ConvTranspose2d(256, num_classes, kernel_size=8, stride=8, padding=0)

    def forward(self, x):
        # å‰å‘ä¼ æ’­ï¼šç‰¹å¾æå–â†’ä¸Šé‡‡æ ·â†’å°ºå¯¸å¯¹é½
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.deconv(x)
        # æ’å€¼å¯¹é½åˆ°256x256ï¼ˆç¡®ä¿å’Œæ ‡ç­¾å°ºå¯¸ä¸€è‡´ï¼‰
        x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=True)
        return x


# ===================== 3. æŸå¤±å‡½æ•°+ä¼˜åŒ–å™¨+æ¨¡å‹è®­ç»ƒ =====================
if __name__ == "__main__":
    # åŸºç¡€é…ç½®
    device = torch.device("cpu")  # æ— æ˜¾å¡ç”¨CPUï¼Œæœ‰æ˜¾å¡æ”¹ä¸º"cuda"
    num_epochs = 5  # è®­ç»ƒè½®æ•°ï¼ˆå¿«é€ŸéªŒè¯æµç¨‹ï¼‰
    batch_size = 4  # æ‰¹é‡å¤§å°
    learning_rate = 1e-4  # å­¦ä¹ ç‡
    save_path = "fcn_seg_model.pth"  # æ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„

    # 1. åŠ è½½æ•°æ®é›†
    train_dataset = FakeSegDataset(num_samples=100)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    print(f"âœ… æ¨¡æ‹Ÿæ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(train_dataset)}å¼ å›¾åƒ")

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = FCN8s(num_classes=21).to(device)
    print("âœ… FCNè¯­ä¹‰åˆ†å‰²æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    # 3. é…ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±ï¼ˆè¯­ä¹‰åˆ†å‰²æ ‡é…ï¼‰
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adamä¼˜åŒ–å™¨

    # 4. æ¨¡å‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
    for epoch in range(num_epochs):
        total_loss = 0.0
        for batch_idx, (imgs, labels) in enumerate(train_loader):
            # æ•°æ®ç§»åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰
            imgs = imgs.to(device)
            labels = labels.to(device)

            # å‰å‘ä¼ æ’­ï¼šæ¨¡å‹é¢„æµ‹
            outputs = model(imgs)

            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­+å‚æ•°æ›´æ–°
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            loss.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°

            total_loss += loss.item()

        # æ‰“å°æ¯è½®è®­ç»ƒç»“æœ
        avg_loss = total_loss / len(train_loader)
        print(f"Epoch [{epoch + 1}/{num_epochs}], å¹³å‡æŸå¤±ï¼š{avg_loss:.4f}")

    # 5. ä¿å­˜æ¨¡å‹æƒé‡ï¼ˆç”Ÿæˆ.pthæ–‡ä»¶ï¼‰
    torch.save(model.state_dict(), save_path)
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³ï¼š{os.path.abspath(save_path)}")

    # 6. å®éªŒæ ¸å¿ƒç»“è®º
    print("\nğŸ“ å®éªŒç»“è®ºï¼š")
    print("1. è¯­ä¹‰åˆ†å‰²æ ¸å¿ƒæ˜¯ã€Œåƒç´ çº§åˆ†ç±»ã€ï¼šæ¨¡å‹è¾“å‡ºæ¯ä¸ªåƒç´ çš„ç±»åˆ«æ¦‚ç‡ï¼ˆæœ¬å®éªŒ21ç±»ï¼‰ï¼›")
    print("2. æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤±è¡¡é‡åƒç´ é¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚ï¼›")
    print("3. è®­ç»ƒé€»è¾‘ï¼šé€šè¿‡åå‘ä¼ æ’­æ›´æ–°å·ç§¯/åå·ç§¯å±‚å‚æ•°ï¼Œé™ä½åˆ†å‰²æŸå¤±ï¼›")
    print("4. FCNæ¨¡å‹å…³é”®ï¼šå…¨å·ç§¯ç»“æ„ï¼ˆæ— å…¨è¿æ¥å±‚ï¼‰+ä¸‹é‡‡æ ·ï¼ˆæç‰¹å¾ï¼‰+ä¸Šé‡‡æ ·ï¼ˆæ¢å°ºå¯¸ï¼‰ã€‚")